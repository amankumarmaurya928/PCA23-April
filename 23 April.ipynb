{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2068549-db82-42a6-91b7-fdb6ceb5bc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The curse of dimensionality basically refers to the difficulties a machine learning algorithm faces when working with\\n   data in the higher dimensions, that did not exist in the lower dimensions. This happens because when you add \\n   dimensions (features), the minimum data requirements also increase rapidly.\\n   It reduces the time and storage space required. It helps Remove multi-collinearity which improves the interpretation \\n   of the parameters of the machine learning model. It becomes easier to visualize the data when reduced to very low \\n   dimensions such as 2D or 3D.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''The curse of dimensionality basically refers to the difficulties a machine learning algorithm faces when working with\n",
    "   data in the higher dimensions, that did not exist in the lower dimensions. This happens because when you add \n",
    "   dimensions (features), the minimum data requirements also increase rapidly.\n",
    "   It reduces the time and storage space required. It helps Remove multi-collinearity which improves the interpretation \n",
    "   of the parameters of the machine learning model. It becomes easier to visualize the data when reduced to very low \n",
    "   dimensions such as 2D or 3D.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf7f959-7676-40b6-bb03-687b7492b082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As the dimensionality increases, the number of data points required for good performance of any machine learning\\n   algorithm increases exponentially. The reason is that, we would need more number of data points for any given\\n   combination of features, for any machine learning model to be valid.\\n   The curse of dimensionality basically refers to the difficulties a machine learning algorithm faces when working with \\n   data in the higher dimensions, that did not exist in the lower dimensions. This happens because when you add \\n   dimensions (features), the minimum data requirements also increase rapidly.\\n   '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''As the dimensionality increases, the number of data points required for good performance of any machine learning\n",
    "   algorithm increases exponentially. The reason is that, we would need more number of data points for any given\n",
    "   combination of features, for any machine learning model to be valid.\n",
    "   The curse of dimensionality basically refers to the difficulties a machine learning algorithm faces when working with \n",
    "   data in the higher dimensions, that did not exist in the lower dimensions. This happens because when you add \n",
    "   dimensions (features), the minimum data requirements also increase rapidly.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df11131-3737-46fd-9bf8-09d073c0dd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As the dimensionality increases, the number of data points required for good performance of any machine learning \\n   algorithm increases exponentially. The reason is that, we would need more number of data points for any given\\n   combination of features, for any machine learning model to be valid.\\n   Both K-mean and K-NN are greatly impacted by the curse of dimensionality, since both of them use the L2 squared\\n   distance measure. As the amount of dimensions increases the distance between various data-points increases as well.\\n   Curse of Dimensionality describes the explosive nature of increasing data dimensions and its resulting exponential \\n   increase in computational efforts required for its processing and/or analysis. This term was first introduced by\\n   Richard E.\\n   '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''As the dimensionality increases, the number of data points required for good performance of any machine learning \n",
    "   algorithm increases exponentially. The reason is that, we would need more number of data points for any given\n",
    "   combination of features, for any machine learning model to be valid.\n",
    "   Both K-mean and K-NN are greatly impacted by the curse of dimensionality, since both of them use the L2 squared\n",
    "   distance measure. As the amount of dimensions increases the distance between various data-points increases as well.\n",
    "   Curse of Dimensionality describes the explosive nature of increasing data dimensions and its resulting exponential \n",
    "   increase in computational efforts required for its processing and/or analysis. This term was first introduced by\n",
    "   Richard E.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4888c93a-0125-4dfe-8101-8ef493886908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While both methods are used for reducing the number of features in a dataset, there is an important difference.\\n   Feature selection is simply selecting and excluding given features without changing them. \\n   Dimensionality reduction transforms features into a lower dimension.\\n   dimension reduction is the process of reducing the number of random variables under consideration, and can be divided\\n   into feature selection and feature extraction.\\n   It avoids the curse of dimensionality. It removes irrelevant features from the data, Because having irrelevant \\n   features in the data can decrease the accuracy of the models and make your model learn based on irrelevant features.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''While both methods are used for reducing the number of features in a dataset, there is an important difference.\n",
    "   Feature selection is simply selecting and excluding given features without changing them. \n",
    "   Dimensionality reduction transforms features into a lower dimension.\n",
    "   dimension reduction is the process of reducing the number of random variables under consideration, and can be divided\n",
    "   into feature selection and feature extraction.\n",
    "   It avoids the curse of dimensionality. It removes irrelevant features from the data, Because having irrelevant \n",
    "   features in the data can decrease the accuracy of the models and make your model learn based on irrelevant features.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ad02ae-f80d-4682-85b0-d53f73904cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Drawbacks of dimensionality reduction:\\n1. We lost some data during the dimensionality reduction process, which can impact how well future training algorithms \\n   work.\\n2. It may need a lot of processing power.\\n3. Interpreting transformed characteristics might be challenging.\\n4. The independent variables become harder to comprehend as a result.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''Drawbacks of dimensionality reduction:\n",
    "1. We lost some data during the dimensionality reduction process, which can impact how well future training algorithms \n",
    "   work.\n",
    "2. It may need a lot of processing power.\n",
    "3. Interpreting transformed characteristics might be challenging.\n",
    "4. The independent variables become harder to comprehend as a result.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ae5b17-67d1-44bc-8f64-ef08f8e0e64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KNN is very susceptible to overfitting due to the curse of dimensionality. Curse of dimensionality also describes the \\n   phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size\\n   training dataset.\\n   Because of this inherent sparsity we end up overfitting, when we add more features to our data, which means we need \\n   more data to avoid sparsity — and that’s the curse of dimensionality: as the number of features increase, our data \\n   become sparser, which results in overfitting, and we therefore need more data to avoid it.\\n   The problem with high dimensional data is that it often requires significant computational resources for modeling,\\n   which could lead to overfitting due to too many parameters or noise due to insufficient examples for each parameter.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''KNN is very susceptible to overfitting due to the curse of dimensionality. Curse of dimensionality also describes the \n",
    "   phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size\n",
    "   training dataset.\n",
    "   Because of this inherent sparsity we end up overfitting, when we add more features to our data, which means we need \n",
    "   more data to avoid sparsity — and that’s the curse of dimensionality: as the number of features increase, our data \n",
    "   become sparser, which results in overfitting, and we therefore need more data to avoid it.\n",
    "   The problem with high dimensional data is that it often requires significant computational resources for modeling,\n",
    "   which could lead to overfitting due to too many parameters or noise due to insufficient examples for each parameter.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c06ca3d-356d-4f24-82a7-cdeaed75c66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dimensionality reduction techniques can be categorized into two broad categories:\\n1. Feature selection. \\n2. Feature extraction. \\n3. Principal Component Analysis (PCA) \\n4. Non-negative matrix factorization (NMF) \\n5. Linear discriminant analysis (LDA) \\n6. Generalized discriminant analysis (GDA) \\n7. Missing Values Ratio.\\n8. Low Variance Filter.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "'''Dimensionality reduction techniques can be categorized into two broad categories:\n",
    "1. Feature selection. \n",
    "2. Feature extraction. \n",
    "3. Principal Component Analysis (PCA) \n",
    "4. Non-negative matrix factorization (NMF) \n",
    "5. Linear discriminant analysis (LDA) \n",
    "6. Generalized discriminant analysis (GDA) \n",
    "7. Missing Values Ratio.\n",
    "8. Low Variance Filter.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d357114-376e-401e-aa18-1743e4844b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
